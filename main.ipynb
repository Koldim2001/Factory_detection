{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных и обучение нейронных сетей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "import torch, torchvision\n",
    "from  matplotlib import pyplot as plt\n",
    "from PIL import Image \n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time \n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms, models\n",
    "import cv2\n",
    "from bs4 import BeautifulSoup\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%matplotlib inline\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для воспроизводимости результатов зафиксируем сиды:\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем обучение нейронной сети, выполняющей детектирование объектов класса \"person\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется заранее размеченный датасет. Закачать можно по этой [ссылке](https://disk.yandex.ru/d/7HNoc81at3r6VQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем аугментацию данных, увеличив датасет вдвое. Для этого написал кастомную функцию aug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import *\n",
    "#aug(out_folder='augmented_dataset')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преподготовка данных для обучения сети Faster RCNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на тренировочные и тестовые:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получим имена файлов без значения кодировки\n",
    "names = []\n",
    "for file in os.listdir('augmented_dataset/images'):\n",
    "    names.append(file.split('.')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рандомно поделим фотографии на train (80%) и test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число изображений на train = 800\n",
      "Число изображений на test = 200\n"
     ]
    }
   ],
   "source": [
    "train_data = random.sample(names, int(len(names) * 0.8))\n",
    "print(f'Число изображений на train = {len(train_data)}')\n",
    "test_data = list(set(names) - set(train_data))\n",
    "print(f'Число изображений на test = {len(test_data)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В файле data_analysis.ipynb мы поработали с данными в формате json. Теперь рассмотрим как работать работать с аналогичными данными, но представленными в формате xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<object>\n",
      "<name>person</name>\n",
      "<truncated>0</truncated>\n",
      "<occluded>0</occluded>\n",
      "<difficult>0</difficult>\n",
      "<bndbox>\n",
      "<xmin>1000.3</xmin>\n",
      "<ymin>301.6</ymin>\n",
      "<xmax>1082.3</xmax>\n",
      "<ymax>514.5</ymax>\n",
      "</bndbox>\n",
      "</object>, <object>\n",
      "<name>person</name>\n",
      "<truncated>0</truncated>\n",
      "<occluded>0</occluded>\n",
      "<difficult>0</difficult>\n",
      "<bndbox>\n",
      "<xmin>1140.9</xmin>\n",
      "<ymin>260.36</ymin>\n",
      "<xmax>1215.6</xmax>\n",
      "<ymax>493.2</ymax>\n",
      "</bndbox>\n",
      "</object>]\n"
     ]
    }
   ],
   "source": [
    "with open('detect_dataset/annotations/PASCAL_VOC_xml/oz7_violation_frame519.xml') as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "        num_objs = len(objects)\n",
    "print(objects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае информативная информация представлена в разделе _xmin_, _ymin_, _xmax_, _ymax_ и сам класс представлен в разделе _name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим функции, которые как раз распарсят эти данные:\n",
    "def generate_box(obj):\n",
    "    xmin = int(float(obj.find('xmin').text))\n",
    "    ymin = int(float(obj.find('ymin').text))\n",
    "    xmax = int(float(obj.find('xmax').text))\n",
    "    ymax = int(float(obj.find('ymax').text))\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "'''\n",
    "класс человек - 1\n",
    "p.s:\n",
    "    Я заранее прописал условия номеров классов\n",
    "    для обучении следующей сети с двумя классами:\n",
    "      1 -  человек с каской\n",
    "      2 -  человек без каски\n",
    "'''\n",
    "def generate_label(obj):\n",
    "    if (obj.find('name').text == \"person\") or (obj.find('name').text == \"hat\"):\n",
    "        return 1\n",
    "    elif obj.find('name').text == \"no_hat\":\n",
    "        return 2   \n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch как раз при обучении для моделей детекции тредует данные в формате [xmin, ymin, xmax, ymax] для каждого бокса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Эта функция будет выдавать на выходе словарь с 3 ключами: boxes, labels и image_id.\n",
    "На вход функция принимает:\n",
    "image_id - индекс фотки из Dataset класса Pytorch \n",
    "file - путь к xml файлу\n",
    "'''\n",
    "def generate_target(image_id, file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        # Будем итерироваться по листу, полученному после раскрытия xml файла:\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # В данном случае всего 1 класс \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # переведем индекс торч тензор\n",
    "        img_id = torch.tensor([image_id])\n",
    "\n",
    "        # получим итоговый словарь для исследуемой фотографии\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = img_id\n",
    "        \n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset(Dataset):\n",
    "    def __init__(self, path, data, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.names_list = data\n",
    "        self.path = path\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names_list[idx]\n",
    "        file_image = self.path + '/images/' + str(name) + '.jpg'\n",
    "        file_label = self.path + '/annotations/' + str(name) + '.xml'\n",
    "        img = Image.open(file_image).convert(\"RGB\")\n",
    "\n",
    "        #Сделаем словарь с аннотацией с помощью ранее написанной функции:\n",
    "        target = generate_target(idx, file_label)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# На вход класса MakeDataset в __init__ подается значение списка файлов и \n",
    "# путь общей папке с аннотациями и изображениями:\n",
    "\n",
    "train_dataset = MakeDataset(path='augmented_dataset', data=train_data, transforms=data_transform)\n",
    "val_dataset = MakeDataset(path='augmented_dataset', data=test_data, transforms=data_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим в каком формате данные харнятся в классе Датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[ 915.,  371., 1064.,  761.]]), 'labels': tensor([1]), 'image_id': tensor([6])}\n"
     ]
    }
   ],
   "source": [
    "first = train_dataset[6]\n",
    "features, labels = first\n",
    "print(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае на фотке есть 2 объекта класса человек, поэтому 2 bounding бокса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):  # Представляет значения в батчах как кортежи\n",
    "    return tuple(zip(*batch)) \n",
    "\n",
    "\n",
    "batch_size = 4  # Зададим чило фотографий на 1 батч\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Демонстрация выходных данных в класса DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 994.,  319., 1069.,  642.],\n",
      "        [ 899.,  378., 1017.,  668.]]), 'labels': tensor([1, 1]), 'image_id': tensor([288])}, {'boxes': tensor([[ 973.,  319., 1083.,  615.],\n",
      "        [ 873.,  361.,  993.,  611.]]), 'labels': tensor([1, 1]), 'image_id': tensor([71])}, {'boxes': tensor([[ 753.,  503.,  883.,  930.],\n",
      "        [ 842.,  471., 1081.,  980.]]), 'labels': tensor([1, 1]), 'image_id': tensor([623])}, {'boxes': tensor([[678., 159., 711., 257.],\n",
      "        [642., 162., 697., 274.]]), 'labels': tensor([1, 1]), 'image_id': tensor([597])}]\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "for imgs, annotations in train_data_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализируем исходные изображения: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем случайные 2 фотки и изобразим их боксы:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_data_loader))\n",
    "\n",
    "# Пройдемся в цикле по двум первым фоткам в рандомном батче:\n",
    "for i, image in enumerate(image_batch[0:2]):\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    img = image.numpy().copy() \n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    annot = label_batch[i]\n",
    "\n",
    "    # Зададим наименования классов:\n",
    "    class_detect = ['none','person']\n",
    "\n",
    "    # Зададим цветовое отображение для классов:\n",
    "    color_class = [(0,0,255), (0,255,0), (255,50,0)]\n",
    "\n",
    "    # Пройдемся в цикле по всем боксам на изображении\n",
    "    for j in range(annot['boxes'].size()[0]):\n",
    "            [xmin, ymin, xmax, ymax] = annot['boxes'][j]\n",
    "            xmin = int(xmin)\n",
    "            ymin = int(ymin)\n",
    "            xmax = int(xmax)\n",
    "            ymax = int(ymax)\n",
    "            n_class = int(annot['labels'][j])\n",
    "            text = class_detect[n_class] \n",
    "            img = cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color_class[n_class], 2)\n",
    "            img = cv2.putText(img, text, (xmin, ymin - 15),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, color_class[n_class], 3)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать Transfer learning подход, обучая сеть Faster RCNN, которая была уже предобучена на COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Заменим число выходных класссов на то, что нам нужно\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Зададим модель (число классов задал как n+1, так как пустой 0 класс)\n",
    "model = get_model_instance_segmentation(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение сети:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутри реализации Faster R-CNN с FPN (Feature Pyramid Network) в библиотеке PyTorch torchvision используется сложная функция потерь, которая объединяет несколько подфункций.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Будем обучать сети на видеокарте:\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "#device = 'cpu'\n",
    "model.to(device)\n",
    "num_epochs = 40  # число эпох обучения"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае выбор оптимизатора SGD может быть обусловлен следующими причинами:\n",
    "\n",
    "Размер и сложность модели: Faster R-CNN с ResNet50-FPN — это достаточно большая и сложная модель с множеством обучаемых параметров, что может привести к быстрому переобучению и нестабильности при использовании более сложных оптимизаторов, таких как Adam.\n",
    "Количество и тип данных: при обучении объектных детекторов, основанных на Faster R-CNN, используется функция потерь, которая состоит из нескольких компонентов, включая компоненты, связанные с классификацией и регрессией объектов. SGD является классическим оптимизатором, который хорошо справляется с обучением таких моделей, в то время как Adam, который является более продвинутым методом, может не давать оптимальных результатов.\n",
    "Наличие предобученных весов: в данном случае мы используем предобученные веса для Faster R-CNN с ResNet50-FPN, что может упростить процесс обучения и позволить использовать более простой оптимизатор SGD вместо Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим пустую папку, в которую будем сохранять обученные модели\n",
    "newpath = 'models'\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f1043785658b2523\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f1043785658b2523\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Запустим прямо в ноутбуке TensorBoard:\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir 'runs'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_dict представляет словарь, который содержит значения функции потерь для каждой из компонентов Faster R-CNN, используемых во время обучения. В частности, словарь loss_dict в модели Faster R-CNN включает следующие компоненты функции потерь:\n",
    "\n",
    "1. Loss_objectness отвечает за определение, содержит ли регион предполагаемого объекта какой-либо объект или нет (бинарная классификация). Для этого loss_objectness использует бинарную кросс-энтропию между выходом сети и соответствующими метками для каждого региона.\n",
    "\n",
    "2. Loss_classifier отвечает за определение, к какому классу принадлежит объект в данном регионе. Для этого loss_classifier использует многоклассовую кросс-энтропию между выходом сети и соответствующими метками классов для каждого региона.\n",
    "3. Loss_box_reg отвечает за определение насколько хорошо модель прогнозирует корректные координаты ограничивающего прямоугольника (bounding box) для обнаруженного объекта на изображении. Для этого loss_box_reg использует среднеквадратичную ошибку между прогнозируемыми координатами ограничивающего прямоугольника и фактическими координатами.\n",
    "\n",
    "4. Loss_rpn_box_reg отвечает за определение насколько хорошо модель прогнозирует координаты ограничивающего прямоугольника (bounding box) для регионов, полученных от Region Proposal Network (RPN), которые могут содержать объекты. Для этого loss_rpn_box_reg использует среднеквадратичную ошибку между прогнозируемыми координатами ограничивающего прямоугольника и фактическими координатами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Код обучения сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Идет обучение 1 эпохи (из 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [01:43, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Инициализация Tensorboard\n",
    "writer = SummaryWriter(comment = ' Human_detection')\n",
    "\n",
    "# Задал значение стартового наивысщего лосса валидации:\n",
    "best_loss = float('inf') \n",
    "\n",
    "# Цикл по эпохам обучения:\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f'Идет обучение {epoch+1} эпохи (из {num_epochs})')   \n",
    "\n",
    "    # Будем считать суммарную ошибку на эпохе, поэтому первоначально занулим лоссы\n",
    "    epoch_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "\n",
    "    # Переведем модель в режим обучения и начнем итераыии по батчам\n",
    "    model.train()\n",
    "    for i, (images, targets) in tqdm(enumerate(train_data_loader)):\n",
    "        \n",
    "        # Стопаю итерацию по батчам после 25 спусков. Из-за suffle=True фотки каждый раз новые\n",
    "        # Благодаря этому мы сможем чаще оценивать loss валидации \n",
    "        if i == 25:\n",
    "            break\n",
    "\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Получим словарь со значниями loss функций :\n",
    "        loss_dict = model(images, targets)       \n",
    "\n",
    "        # Найдем суммарный loss и сделаем шаг град. спуска:\n",
    "        losses = sum(loss for loss in loss_dict.values())       \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        # Используем для подсчета всехсредних лоссов за эпоху:\n",
    "        loss_classifier += loss_dict['loss_classifier'].item()\n",
    "        loss_box_reg += loss_dict['loss_box_reg'].item()\n",
    "        loss_objectness += loss_dict['loss_objectness'].item()\n",
    "        loss_rpn_box_reg += loss_dict['loss_rpn_box_reg'].item()\n",
    "        epoch_loss += losses\n",
    "\n",
    "    # Запишем в TensorBoard значения лоссов на трейне:    \n",
    "    writer.add_scalar('Summ train loss during epochs', epoch_loss/i, epoch+1)\n",
    "    writer.add_scalar('Train loss_classifier', loss_classifier/i, epoch+1)\n",
    "    writer.add_scalar('Train loss_box_reg', loss_box_reg/i, epoch+1)\n",
    "    writer.add_scalar('Train loss_objectness', loss_objectness/i, epoch+1)\n",
    "    writer.add_scalar('Train loss_rpn_box_reg', loss_rpn_box_reg/i, epoch+1)\n",
    "\n",
    "    print(f'Train summ loss after {epoch+1} epochs = {epoch_loss/i}')\n",
    "\n",
    "    # Обнулим суммарный лосс валидации\n",
    "    val_total_loss = 0\n",
    "    # Перевод модели в режим валидации для оценки суммарного лосса на вал. датасете\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_data_loader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets) \n",
    "\n",
    "            # Вычислим лосс на батче\n",
    "            losses = sum(loss for loss in loss_dict.values())        \n",
    "            val_total_loss += losses.item()\n",
    "\n",
    "        # Запишем в TensorBoard значение суммарного лосса на валидации:\n",
    "        writer.add_scalar('Summ validation loss during epochs',\n",
    "                          val_total_loss/len(val_data_loader) , epoch+1)\n",
    "        print(f'Validation summ loss after {epoch+1} epochs = {val_total_loss/len(val_data_loader)}')\n",
    "\n",
    "    '''\n",
    "    Если на валидации мы получили меньше лосс чем текущий best_loss, то сохраняем веса модели\n",
    "    Тем самым мы на выходе получим модель, имеющую самый низкий лосс на валидации, \n",
    "    из всех обученных нами эпох\n",
    "    '''\n",
    "    if val_total_loss/len(val_data_loader) < best_loss:\n",
    "        state_dict = model.state_dict()\n",
    "        print('Сохраним новую модель, так как текущая конфигурация имеет ниже val loss')\n",
    "        best_loss = val_total_loss/len(val_data_loader)\n",
    "        torch.save(state_dict, 'models/model_human_detection.pth')\n",
    "        \n",
    "# Закрытие Tensorboard\n",
    "writer.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Идет обучение 1 эпохи (из 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:19, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9148/2779336985.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtraining_rcnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m train(model=model, train_data_loader=train_data_loader, optimizer=optimizer,\n\u001b[0m\u001b[0;32m      3\u001b[0m       \u001b[0mval_data_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       num_epochs=2, comment='test', device=device)\n",
      "\u001b[1;32mc:\\Users\\User\\repository_Koldim2001\\Hardhat_detector\\training_rcnn.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_data_loader, optimizer, val_data_loader, num_epochs, comment, device)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m# Получим словарь со значниями loss функций :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m# Найдем суммарный loss и сделаем шаг град. спуска:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     99\u001b[0m                     )\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torchvision\\ops\\feature_pyramid_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0minner_top_down\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_inner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeat_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"nearest\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mlast_inner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner_lateral\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minner_top_down\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result_from_layer_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_inner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextra_blocks\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torchvision\\ops\\feature_pyramid_network.py\u001b[0m in \u001b[0;36mget_result_from_layer_blocks\u001b[1;34m(self, x, idx)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from training_rcnn import *\n",
    "train(model=model, train_data_loader=train_data_loader, optimizer=optimizer,\n",
    "      val_data_loader=val_data_loader,\n",
    "      num_epochs=2, comment='test', device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка самого успешного состояния модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(2)\n",
    "model.load_state_dict(torch.load('models/model_human_detection.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "тестирование:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Program Files (x86)\\Arduino\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объектов класса person обнаружено 1\n"
     ]
    }
   ],
   "source": [
    "from predict import *\n",
    "detect_and_visualize(image_input='detect_dataset/images/am3_7_violation_frame142.jpg', classes=['person'], plt_show=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1080, 1920])\n"
     ]
    }
   ],
   "source": [
    "name = test_data[12]  # Взяли фотку с теста\n",
    "file_image = 'augmented_dataset/images/' + str(name) + '.jpg'\n",
    "image = Image.open(file_image).convert(\"RGB\")\n",
    "test_im = data_transform(image)\n",
    "device = 'cpu'\n",
    "test_im = test_im[ None, :, :, :]\n",
    "print(test_im.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 728.4958,  213.5045,  764.2094,  259.7223],\n",
      "        [ 716.9158,  197.8254,  769.2668,  271.3576],\n",
      "        [ 723.1696,  190.7163,  751.8546,  251.4628],\n",
      "        [ 739.5417,  190.8153,  765.3412,  258.1391],\n",
      "        [ 722.2820,  229.2909,  768.1446,  283.6917],\n",
      "        [ 737.2021,  230.2848,  763.5924,  264.9203],\n",
      "        [ 720.2897,  209.1976,  749.8693,  288.8727],\n",
      "        [ 695.4041,  190.0727,  769.3415,  296.4543],\n",
      "        [ 743.9470,  223.0480,  769.2274,  270.9832],\n",
      "        [ 571.6551,  165.4066,  627.0245,  235.6716],\n",
      "        [ 744.6705,  220.4452,  775.5914,  253.0870],\n",
      "        [ 715.5301,  172.0306,  751.2354,  228.9783],\n",
      "        [ 748.5699,  211.3569,  790.3383,  265.2649],\n",
      "        [ 729.3192,  165.5235,  767.5104,  208.2189],\n",
      "        [ 719.0078,  197.9938,  761.0037,  334.4930],\n",
      "        [ 473.9755,  178.2271,  659.3376,  237.0885],\n",
      "        [ 747.1430,  242.9357,  771.5836,  271.2579],\n",
      "        [ 740.7446,  246.7722,  764.3783,  276.9828],\n",
      "        [ 559.6323,  128.5594,  633.8876,  212.2978],\n",
      "        [ 743.9676,  179.6375,  779.7523,  274.2063],\n",
      "        [ 717.9064,  195.8490,  741.0523,  276.6261],\n",
      "        [ 716.7147,  163.0682,  782.2382,  244.1654],\n",
      "        [ 716.8149,  196.9532,  798.6812,  306.2846],\n",
      "        [ 578.1077,  131.9431,  614.6778,  205.4998],\n",
      "        [ 508.7932,  136.2046,  649.0578,  213.9993],\n",
      "        [ 767.4758,  346.4238,  846.6880,  435.2870],\n",
      "        [ 649.8751,  209.1286,  826.7792,  277.3774],\n",
      "        [ 578.7751,  145.3609,  606.8714,  230.8794],\n",
      "        [ 749.4023,  256.3868,  774.8837,  279.1043],\n",
      "        [ 586.5783,  147.8483,  627.9730,  220.2325],\n",
      "        [ 541.2889,  170.4615,  591.2213,  327.3481],\n",
      "        [1479.8374,   27.2654, 1920.0000,  353.4095],\n",
      "        [ 752.6620,  226.2419,  773.7435,  263.7411],\n",
      "        [ 701.0514,  156.9606,  750.0318,  343.4722],\n",
      "        [ 180.2398,   84.4970,  714.8903,  569.2938],\n",
      "        [ 206.7719,  721.4247,  919.0190, 1057.7097],\n",
      "        [ 582.4139,  130.1317,  622.4743,  175.6425],\n",
      "        [ 734.2134,  127.0763,  785.6837,  282.0984],\n",
      "        [ 335.3108,  197.0434,  705.8655,  620.1571],\n",
      "        [ 568.0936,  367.0033, 1321.4044, 1080.0000],\n",
      "        [ 682.0158,  139.1954,  778.1036,  366.6323],\n",
      "        [ 466.2496,   14.6582,  536.0147,  196.4299],\n",
      "        [1227.8921,  579.7787, 1598.9862, 1034.8103],\n",
      "        [ 436.8342,  101.2264, 1240.5597,  432.8167],\n",
      "        [ 190.2873,  282.7076,  205.8862,  338.2281],\n",
      "        [ 454.0320,  566.1056,  810.4372, 1016.1916],\n",
      "        [ 423.4284,  255.5860,  511.5173,  340.6185],\n",
      "        [ 272.9776,  137.5894,  599.5591,  781.1570],\n",
      "        [ 297.1508,   17.4075,  705.4993,  397.1496],\n",
      "        [   0.0000,   95.7679,  999.1194,  724.3394],\n",
      "        [ 757.9610,  280.5703,  840.6331,  421.1039],\n",
      "        [ 760.1920,  227.7682,  784.5394,  265.6135],\n",
      "        [ 807.4956,  176.5130,  854.8626,  394.3707],\n",
      "        [ 803.1075,  376.1033,  828.0327,  429.0045],\n",
      "        [ 551.9695,  155.6159,  581.4635,  231.7875],\n",
      "        [1009.5435,  399.3901, 1432.9821,  837.8072],\n",
      "        [  40.2319,  233.1342,  635.6895,  741.0536],\n",
      "        [ 749.9515,  230.8569,  773.5208,  297.3162],\n",
      "        [ 758.9750,  256.0078,  780.5818,  278.5158],\n",
      "        [ 540.9409,  154.1463,  565.4352,  206.5184],\n",
      "        [1305.3239,  697.1343, 1920.0000, 1080.0000],\n",
      "        [ 590.6913,  150.1900,  654.0999,  333.7350],\n",
      "        [ 978.4605,  446.3573, 1920.0000, 1069.9352],\n",
      "        [ 541.4801,  152.8790,  552.2155,  183.8876],\n",
      "        [ 836.3630,  409.2389,  862.6215,  446.1524],\n",
      "        [ 797.3097,  348.3260,  830.0990,  423.4741],\n",
      "        [  35.8983,  488.9079,  993.2988, 1080.0000],\n",
      "        [ 532.7076,  123.2168,  654.6128,  182.7052],\n",
      "        [ 215.3111,  293.5663,  227.3715,  312.6081],\n",
      "        [ 594.8259,  162.9620,  630.8649,  245.2419],\n",
      "        [   0.0000,  203.2927, 1402.5615,  914.1212],\n",
      "        [ 815.4128,  225.9477,  864.6697,  444.0564],\n",
      "        [ 588.1956,  169.4771,  817.4547,  987.7535],\n",
      "        [ 548.0234,  599.9062,  773.2689,  858.6062],\n",
      "        [ 255.1203,   27.8232, 1647.1692,  538.6266],\n",
      "        [ 613.6387,  156.2320,  633.0072,  224.5296],\n",
      "        [1186.3416,  580.0161, 1288.4376,  683.6776],\n",
      "        [ 706.5648,  179.9396, 1358.2922,  761.1470],\n",
      "        [ 709.2326,  303.6392,  948.9541, 1072.1019],\n",
      "        [1153.2544,    0.0000, 1920.0000,  383.6030],\n",
      "        [ 837.3876,  252.7266,  888.4536,  437.4546],\n",
      "        [ 553.7668,  125.0252,  657.7258,  381.1303],\n",
      "        [   0.0000,  235.3147,  498.0298,  463.4752],\n",
      "        [ 441.0495,  271.1011,  580.9285,  350.2908],\n",
      "        [1120.9883,  370.0109, 1239.9966,  680.4899],\n",
      "        [ 608.8703,  175.2959,  630.7245,  231.9710],\n",
      "        [ 511.7398,  154.2504,  529.9106,  190.9345],\n",
      "        [ 795.3342,   76.8007, 1920.0000,  544.5263],\n",
      "        [1055.7428,  194.5472, 1613.3851, 1065.7770],\n",
      "        [ 637.7975,  593.5031,  744.9145, 1080.0000],\n",
      "        [ 601.3395,  504.3210,  705.2977, 1002.9543],\n",
      "        [ 457.8998,  344.6745,  572.6800,  366.7823],\n",
      "        [ 739.6813,  213.8202,  773.3757,  333.5217],\n",
      "        [ 565.5998,  143.5740,  622.8384,  269.5114],\n",
      "        [ 495.9858,    0.0000,  951.6196, 1080.0000],\n",
      "        [1102.1207,  159.3612, 1920.0000,  734.2126],\n",
      "        [  66.5397,  113.9239,  757.4689,  385.3999],\n",
      "        [ 822.5226,  433.8611,  868.6906,  467.4484],\n",
      "        [ 535.3046,  155.6760,  553.8425,  193.0024],\n",
      "        [1673.3788,  156.5375, 1920.0000,  322.3246]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), 'scores': tensor([0.7853, 0.7852, 0.7841, 0.7755, 0.7742, 0.7548, 0.7391, 0.7367, 0.7286,\n",
      "        0.7220, 0.7192, 0.7117, 0.7079, 0.7035, 0.6997, 0.6938, 0.6897, 0.6890,\n",
      "        0.6886, 0.6845, 0.6828, 0.6811, 0.6804, 0.6710, 0.6706, 0.6689, 0.6654,\n",
      "        0.6615, 0.6562, 0.6558, 0.6533, 0.6492, 0.6438, 0.6412, 0.6379, 0.6377,\n",
      "        0.6330, 0.6301, 0.6294, 0.6281, 0.6260, 0.6244, 0.6241, 0.6241, 0.6233,\n",
      "        0.6223, 0.6210, 0.6204, 0.6193, 0.6191, 0.6187, 0.6186, 0.6168, 0.6160,\n",
      "        0.6142, 0.6139, 0.6117, 0.6113, 0.6109, 0.6098, 0.6098, 0.6094, 0.6090,\n",
      "        0.6086, 0.6080, 0.6076, 0.6065, 0.6063, 0.6062, 0.6034, 0.6022, 0.6011,\n",
      "        0.6003, 0.6002, 0.5996, 0.5986, 0.5976, 0.5976, 0.5969, 0.5968, 0.5948,\n",
      "        0.5935, 0.5926, 0.5922, 0.5921, 0.5921, 0.5910, 0.5903, 0.5902, 0.5897,\n",
      "        0.5896, 0.5893, 0.5880, 0.5877, 0.5872, 0.5865, 0.5856, 0.5855, 0.5852,\n",
      "        0.5848])}]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(test_im)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1080, 1920, 3])\n",
      "[{'boxes': tensor([[ 728.4958,  213.5045,  764.2094,  259.7223],\n",
      "        [ 716.9158,  197.8254,  769.2668,  271.3576],\n",
      "        [ 723.1696,  190.7163,  751.8546,  251.4628],\n",
      "        [ 739.5417,  190.8153,  765.3412,  258.1391],\n",
      "        [ 722.2820,  229.2909,  768.1446,  283.6917],\n",
      "        [ 737.2021,  230.2848,  763.5924,  264.9203],\n",
      "        [ 720.2897,  209.1976,  749.8693,  288.8727],\n",
      "        [ 695.4041,  190.0727,  769.3415,  296.4543],\n",
      "        [ 743.9470,  223.0480,  769.2274,  270.9832],\n",
      "        [ 571.6551,  165.4066,  627.0245,  235.6716],\n",
      "        [ 744.6705,  220.4452,  775.5914,  253.0870],\n",
      "        [ 715.5301,  172.0306,  751.2354,  228.9783],\n",
      "        [ 748.5699,  211.3569,  790.3383,  265.2649],\n",
      "        [ 729.3192,  165.5235,  767.5104,  208.2189],\n",
      "        [ 719.0078,  197.9938,  761.0037,  334.4930],\n",
      "        [ 473.9755,  178.2271,  659.3376,  237.0885],\n",
      "        [ 747.1430,  242.9357,  771.5836,  271.2579],\n",
      "        [ 740.7446,  246.7722,  764.3783,  276.9828],\n",
      "        [ 559.6323,  128.5594,  633.8876,  212.2978],\n",
      "        [ 743.9676,  179.6375,  779.7523,  274.2063],\n",
      "        [ 717.9064,  195.8490,  741.0523,  276.6261],\n",
      "        [ 716.7147,  163.0682,  782.2382,  244.1654],\n",
      "        [ 716.8149,  196.9532,  798.6812,  306.2846],\n",
      "        [ 578.1077,  131.9431,  614.6778,  205.4998],\n",
      "        [ 508.7932,  136.2046,  649.0578,  213.9993],\n",
      "        [ 767.4758,  346.4238,  846.6880,  435.2870],\n",
      "        [ 649.8751,  209.1286,  826.7792,  277.3774],\n",
      "        [ 578.7751,  145.3609,  606.8714,  230.8794],\n",
      "        [ 749.4023,  256.3868,  774.8837,  279.1043],\n",
      "        [ 586.5783,  147.8483,  627.9730,  220.2325],\n",
      "        [ 541.2889,  170.4615,  591.2213,  327.3481],\n",
      "        [1479.8374,   27.2654, 1920.0000,  353.4095],\n",
      "        [ 752.6620,  226.2419,  773.7435,  263.7411],\n",
      "        [ 701.0514,  156.9606,  750.0318,  343.4722],\n",
      "        [ 180.2398,   84.4970,  714.8903,  569.2938],\n",
      "        [ 206.7719,  721.4247,  919.0190, 1057.7097],\n",
      "        [ 582.4139,  130.1317,  622.4743,  175.6425],\n",
      "        [ 734.2134,  127.0763,  785.6837,  282.0984],\n",
      "        [ 335.3108,  197.0434,  705.8655,  620.1571],\n",
      "        [ 568.0936,  367.0033, 1321.4044, 1080.0000],\n",
      "        [ 682.0158,  139.1954,  778.1036,  366.6323],\n",
      "        [ 466.2496,   14.6582,  536.0147,  196.4299],\n",
      "        [1227.8921,  579.7787, 1598.9862, 1034.8103],\n",
      "        [ 436.8342,  101.2264, 1240.5597,  432.8167],\n",
      "        [ 190.2873,  282.7076,  205.8862,  338.2281],\n",
      "        [ 454.0320,  566.1056,  810.4372, 1016.1916],\n",
      "        [ 423.4284,  255.5860,  511.5173,  340.6185],\n",
      "        [ 272.9776,  137.5894,  599.5591,  781.1570],\n",
      "        [ 297.1508,   17.4075,  705.4993,  397.1496],\n",
      "        [   0.0000,   95.7679,  999.1194,  724.3394],\n",
      "        [ 757.9610,  280.5703,  840.6331,  421.1039],\n",
      "        [ 760.1920,  227.7682,  784.5394,  265.6135],\n",
      "        [ 807.4956,  176.5130,  854.8626,  394.3707],\n",
      "        [ 803.1075,  376.1033,  828.0327,  429.0045],\n",
      "        [ 551.9695,  155.6159,  581.4635,  231.7875],\n",
      "        [1009.5435,  399.3901, 1432.9821,  837.8072],\n",
      "        [  40.2319,  233.1342,  635.6895,  741.0536],\n",
      "        [ 749.9515,  230.8569,  773.5208,  297.3162],\n",
      "        [ 758.9750,  256.0078,  780.5818,  278.5158],\n",
      "        [ 540.9409,  154.1463,  565.4352,  206.5184],\n",
      "        [1305.3239,  697.1343, 1920.0000, 1080.0000],\n",
      "        [ 590.6913,  150.1900,  654.0999,  333.7350],\n",
      "        [ 978.4605,  446.3573, 1920.0000, 1069.9352],\n",
      "        [ 541.4801,  152.8790,  552.2155,  183.8876],\n",
      "        [ 836.3630,  409.2389,  862.6215,  446.1524],\n",
      "        [ 797.3097,  348.3260,  830.0990,  423.4741],\n",
      "        [  35.8983,  488.9079,  993.2988, 1080.0000],\n",
      "        [ 532.7076,  123.2168,  654.6128,  182.7052],\n",
      "        [ 215.3111,  293.5663,  227.3715,  312.6081],\n",
      "        [ 594.8259,  162.9620,  630.8649,  245.2419],\n",
      "        [   0.0000,  203.2927, 1402.5615,  914.1212],\n",
      "        [ 815.4128,  225.9477,  864.6697,  444.0564],\n",
      "        [ 588.1956,  169.4771,  817.4547,  987.7535],\n",
      "        [ 548.0234,  599.9062,  773.2689,  858.6062],\n",
      "        [ 255.1203,   27.8232, 1647.1692,  538.6266],\n",
      "        [ 613.6387,  156.2320,  633.0072,  224.5296],\n",
      "        [1186.3416,  580.0161, 1288.4376,  683.6776],\n",
      "        [ 706.5648,  179.9396, 1358.2922,  761.1470],\n",
      "        [ 709.2326,  303.6392,  948.9541, 1072.1019],\n",
      "        [1153.2544,    0.0000, 1920.0000,  383.6030],\n",
      "        [ 837.3876,  252.7266,  888.4536,  437.4546],\n",
      "        [ 553.7668,  125.0252,  657.7258,  381.1303],\n",
      "        [   0.0000,  235.3147,  498.0298,  463.4752],\n",
      "        [ 441.0495,  271.1011,  580.9285,  350.2908],\n",
      "        [1120.9883,  370.0109, 1239.9966,  680.4899],\n",
      "        [ 608.8703,  175.2959,  630.7245,  231.9710],\n",
      "        [ 511.7398,  154.2504,  529.9106,  190.9345],\n",
      "        [ 795.3342,   76.8007, 1920.0000,  544.5263],\n",
      "        [1055.7428,  194.5472, 1613.3851, 1065.7770],\n",
      "        [ 637.7975,  593.5031,  744.9145, 1080.0000],\n",
      "        [ 601.3395,  504.3210,  705.2977, 1002.9543],\n",
      "        [ 457.8998,  344.6745,  572.6800,  366.7823],\n",
      "        [ 739.6813,  213.8202,  773.3757,  333.5217],\n",
      "        [ 565.5998,  143.5740,  622.8384,  269.5114],\n",
      "        [ 495.9858,    0.0000,  951.6196, 1080.0000],\n",
      "        [1102.1207,  159.3612, 1920.0000,  734.2126],\n",
      "        [  66.5397,  113.9239,  757.4689,  385.3999],\n",
      "        [ 822.5226,  433.8611,  868.6906,  467.4484],\n",
      "        [ 535.3046,  155.6760,  553.8425,  193.0024],\n",
      "        [1673.3788,  156.5375, 1920.0000,  322.3246]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), 'scores': tensor([0.7853, 0.7852, 0.7841, 0.7755, 0.7742, 0.7548, 0.7391, 0.7367, 0.7286,\n",
      "        0.7220, 0.7192, 0.7117, 0.7079, 0.7035, 0.6997, 0.6938, 0.6897, 0.6890,\n",
      "        0.6886, 0.6845, 0.6828, 0.6811, 0.6804, 0.6710, 0.6706, 0.6689, 0.6654,\n",
      "        0.6615, 0.6562, 0.6558, 0.6533, 0.6492, 0.6438, 0.6412, 0.6379, 0.6377,\n",
      "        0.6330, 0.6301, 0.6294, 0.6281, 0.6260, 0.6244, 0.6241, 0.6241, 0.6233,\n",
      "        0.6223, 0.6210, 0.6204, 0.6193, 0.6191, 0.6187, 0.6186, 0.6168, 0.6160,\n",
      "        0.6142, 0.6139, 0.6117, 0.6113, 0.6109, 0.6098, 0.6098, 0.6094, 0.6090,\n",
      "        0.6086, 0.6080, 0.6076, 0.6065, 0.6063, 0.6062, 0.6034, 0.6022, 0.6011,\n",
      "        0.6003, 0.6002, 0.5996, 0.5986, 0.5976, 0.5976, 0.5969, 0.5968, 0.5948,\n",
      "        0.5935, 0.5926, 0.5922, 0.5921, 0.5921, 0.5910, 0.5903, 0.5902, 0.5897,\n",
      "        0.5896, 0.5893, 0.5880, 0.5877, 0.5872, 0.5865, 0.5856, 0.5855, 0.5852,\n",
      "        0.5848])}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_im = test_im[0].to(device)\n",
    "test_im = np.transpose(test_im,(1,2,0))\n",
    "print(test_im.size())\n",
    "test_im = test_im.numpy()\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Реализация построения результата детекции тестовой картинки\n",
    "treshhold = 0.1  # Задалим порог. Если score больше порога, то выделяем бокс\n",
    "plt.figure(figsize=(9, 7), dpi=80)\n",
    "plt.title(f'test_image')\n",
    "image = (test_im.copy() * 255).astype(np.uint8)\n",
    "class_detect = ['none','person']\n",
    "\n",
    "for i in range(preds[0]['boxes'].size()[0]):\n",
    "        if float(preds[0]['scores'][i]) > treshhold:\n",
    "                [xmin, ymin, xmax, ymax] = preds[0]['boxes'][i]\n",
    "                xmin = int(xmin)\n",
    "                ymin = int(ymin)\n",
    "                xmax = int(xmax)\n",
    "                ymax = int(ymax)\n",
    "                text = class_detect[int(preds[0]['labels'][i])] + ' ' + \\\n",
    "                  str(round(float(preds[0]['scores'][i]),2))\n",
    "                image = cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "                image = cv2.putText(image, text, (xmin, ymin - 15),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детекция людей с каской с без:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью вэб сервиса https://www.makesense.ai/ я самостоятельно разметил часть изображений, сделав на этот раз 2 отдельных класса - __человек с каской на голове__ и __человек без каски__. <br>\n",
    "Датасет, содержащий новую аннотацию можно скачать по этой [ссылке](https://disk.yandex.ru/d/9fph4DZZbUc1bQ) <br>\n",
    "Результирующие аннотации я сохранил в папке detect_hat_dataset/annotations <br><br>\n",
    "\n",
    "Теперь добавим в пока еще пустую папку detect_hat_dataset/images изображения, соответсвующие данным аннотациям (отберем по совпадающему названию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего вручную размеченных фотографий 128 штук\n"
     ]
    }
   ],
   "source": [
    "# Получим имена файлов аннотаций нового датасета без значения кодировки\n",
    "names = []\n",
    "for file in os.listdir('detect_hat_dataset/annotations'):\n",
    "    names.append(file.split('.')[0])\n",
    "print(f'Всего вручную размеченных фотографий {len(names)} штук')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скопируем из папки detect_dataset/images подходящие фотки в папку detect_hat_dataset/images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('detect_dataset/images'):\n",
    "    if file.split('.')[0] in names:\n",
    "        shutil.copy2('detect_dataset/images/' + file, 'detect_hat_dataset/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как в данном случае изображений размеченных особенно мало, поэтому воспользуемся готовой функцией аугментации для увеличения размера датасета в 2 раза:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное число фотографий и аннотаций = 128\n",
      "Итоговое число фотографий и аннотаций = 256\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "aug(image_dir=\"detect_hat_dataset/images\",\n",
    "                  xml_dir=\"detect_hat_dataset/annotations\",\n",
    "                  out_folder='augmented_hat_dataset')\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть папка augmented_hat_dataset с которой мы и будем работать при обучении сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
